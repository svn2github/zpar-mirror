\documentclass[12pt]{article}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{hyperref}
\usepackage{CJK}
\usepackage{times}
\makeindex
\title{English POS Tagging}
\begin{document}
\begin{CJK}{GBK}{song}
\maketitle

\section{How to compile}
Suppose that Zpar has been downloaded to the directory \textit{zpar}. To make a POS tagging system for English, type \textit{make english.postagger}. This will create a directory \textit{zpar/dist/english.postagger}, in which there are two files: \textit{train} and \textit{tagger}. The file \textit{train} is used to train a tagging model,and the file \textit{tagger} is used to tag new texts using a trained parsing model. 
\section{Format of inputs and outputs}
The input file to the \textit{train} file contains a set of labeled sentences, one for each line. An example is:
\\
\\
%\begin{tabular}{l}
\hspace{3cm} Ms./NNP Haag/NNP plays/VBZ Elianti/NNP ./.
\\
\\
%\end{tabular}
The format is different from the original format used in Penn Treebanks. Here is a \href{con_files/binarize.py}{Python script} to convert the original Penn Treebank format to the Zpar format. The usage is
\\
\\
\hspace{3cm} python binarize.py $<$rule-file$>$ $<$input-file$>$ \\
\\
Here \textit{rule-file} is a file containing head-finding rules (see the \href{con_files/rule.txt}{example rules} for Penn Chinese Treebank), and the conversion results will be printed to the console. Note that, in the respect of Chinese, the encoding of input-file to \textit{binarize.py} should be \textit{gb} and the output will be encoded in \textit{utf8}. Here is a \href{seg_files/gb2utf.py}{script} that transfers files that are encoded in \textit{gb} to the \textit{utf8} encoding.
\\
\\
The input file to the \textit{conparser} contain POS tagged sentences. The formats for English and Chinese are different.
\\
\\
\hspace{2cm} \textbf{English}: \\
\hspace{3cm} Ms./NNP Haag/NNP plays/VBZ Elianti/NNP\\
\hspace{2cm}\textbf{Chinese}: \\
\hspace{3cm} ZPar$\_$NR 可以$\_$MD 分析$\_$VV 中文$\_$NN 和$\_$CC 英文$\_$NN \\
\\
For Chinese, inputs to both \textit{train} and \textit{conparser} must be encoded in \textit{utf8}.
\section{How to train a model}
To train a model, use
\\
\\
\hspace{3cm} zpar/dist/english.conparser/train $<$train-file$>$ $<$model-file$>$ $<$number of iterations$>$ \\
\\
For example, using the \href{con_files/train.txt}{train file}, you can train a  model by
\\
\\
\hspace{3cm} zpar/dist/english.conparser/train train.txt model 1 \\
\\
After training is completed, a new file \textit{model} will be created in the current directory, which can be used to parse POS-tagged sentences. The above command performs training with one iteration using the training file. The commands for training Chinese parsing models are the same.
\section{How to parse new texts}
To apply an existing model to parse new texts, use
\\
\\
\hspace{3cm} zpar/dist/english.conparser/conparser $<$model$>$ $<$input-file$>$ $<$output-file$>$
\\
\\
For example, using the model we just trained, we can parse \href{con_files/input.txt}{an example input} by
\\
\\
\hspace{3cm} zpar/dist/english.conparser/conparser model input.txt output.txt
\\
\\
The output file contains automatically parsed trees. The commands for parsing Chinese texts are the same.
\section{Outputs and evaluation}
In order to evaluate the quality of the outputs, we can manually specify the gold parse trees of a sample, and compare the outputs with the correct sample.
\\
Manually specified parse trees of the input file are given in \href{con_files/reference.txt}{reference file}. Refer to \href{http://nlp.cs.nyu.edu/evalb/}{evalb} to obtain a software that performs automatic evaluation.
\\
Using the above \textit{output.txt} and \textit{reference.txt}, we can evaluate the accuracies by typing
\\
\\
\hspace{3cm} ./evalb -p $<$config.file$>$ output.txt reference.txt
\\
\\
Here \textit{config.file} sets running parameters of the evaluation. \href{con_files/COLLINS.prm}{COLLINS.prm} is a widely used configuration file.
Evaluation results will be printed to the console.
\section{How to tune the performance of a system}
The performance of the system after one training iteration may not be optimal. You can try training a model for another few iterations, after each you compare the performance. You can choose the model that gives the highest f-score on your test data. We conventionally call this test file the development test data, because you develop a parsing model using this. Here is a \href{con_files/test.sh}{a shell script} that automatically trains the parser for 30 iterations, and after the $i$th iteration, stores the model file to model.$i$. You can compare the f-score of all 30 iterations and choose model.$k$, which gives the best f-score, as the final model. In this file, this is a variable called \textit{parser}. You need to set this variable to the relative directory of \textit{zpar/dist/english.conparsr} or \textit{zpar/dist/chinese.conparser}.
%\begin{thebibliography}{99}
%\bibitem{bib-1}
%Yue Zhang and Stephen Clark. 2011. Syntactic Processing Using the Generalized Perceptron and Beam Search. \textit{Computational Linguistics}, 37(1):105-151.
%Yue Zhang and Stephen Clark. 2009. Transition-Based Parsing of the Chinese Treebank using a Global Discriminative Model. In {\em Proc. of IWPT} 2009, pages 162-171.
%\end{thebibliography}
\end{CJK}
\end{document}
